{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZW2BTg-FqO8X"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xk4AgX7pld5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "'''\n",
    "def generate_words(dataset):\n",
    "\twords = []\n",
    "\tfor data in dataset:\n",
    "\t\tfor word in nltk.word_tokenize(data):\n",
    "\t\t\twords.append(word)\n",
    "\treturn words\n",
    "'''\n",
    "def clean_data(dataset):\n",
    "\tfor i in range(len(dataset)):\n",
    "\t    dataset[i] = dataset[i].lower()\n",
    "\t    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "\t    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
    "\t    dataset[i] = re.sub('^\\s+', '', dataset[i])\n",
    "\t    dataset[i] = re.sub(r'\\s$', '', dataset[i])\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "def create_word_count(words):\n",
    "\tword_count = {}\n",
    "\tfor word in words:\n",
    "\t\tif word in word_count.keys():\n",
    "\t\t\tword_count[word] += 1\n",
    "\t\telse:\n",
    "\t\t\tword_count[word] = 1\n",
    "\treturn word_count\n",
    "\n",
    "def create_idf(dataset, words):\n",
    "\tidf = {}\n",
    "\tfor word in words:\n",
    "\t    t = 0\n",
    "\t    for data in dataset:\n",
    "\t        if word in data:\n",
    "\t            t+=1;\n",
    "\t    idf[word] = np.log((len(dataset)+1)/(t+1))\n",
    "\treturn idf\n",
    "\n",
    "def create_tf(dataset, words):\n",
    "\t'''\n",
    "\ttf = []\n",
    "\tfor i in range(len(dataset)):\n",
    "\t    vector = []\n",
    "\t    w = nltk.word_tokenize(dataset[i])\n",
    "\t    for j in range(len(most_freq_words)):\n",
    "\t        vector.append(w.count(most_freq_words[j]) / len(w))\n",
    "\t    tf.append(vector)\n",
    "\ttf = np.asarray(tf)\n",
    "\treturn tf\n",
    "\t'''\n",
    "\ttf = {}\n",
    "\tfor word in words:\n",
    "\t\tdoc_tf = []\n",
    "\t\tfor data in dataset:\n",
    "\t\t\tfreq = 0\n",
    "\t\t\tfor w in nltk.word_tokenize(data):\n",
    "\t\t\t\tif w == word:\n",
    "\t\t\t\t\tfreq += 1\n",
    "\t\t\tdoc_tf.append(freq/len(nltk.word_tokenize(data)))\n",
    "\t\ttf[word] = doc_tf\n",
    "\treturn tf\n",
    "\n",
    "def create_tfidf(dataset, most_freq_words):\n",
    "\ttf = create_tf(dataset, most_freq_words)\n",
    "\tidf = create_idf(dataset, most_freq_words)\n",
    "\ttfidf = []\n",
    "\tfor word in tf.keys():\n",
    "\t    t = []\n",
    "\t    for value in tf[word]:\n",
    "\t        t.append(idf[word]*value)\n",
    "\t    tfidf.append(t)\n",
    "\ttfidf = np.asarray(tfidf)\n",
    "\ttfidf = np.transpose(tfidf)\n",
    "\treturn tfidf\n",
    "\n",
    "def get_sorted_words(tfidf, words):\n",
    "\ts = {}\n",
    "\tfor i in range(tfidf.shape[0]):\n",
    "\t    for j in range(tfidf.shape[1]):\n",
    "\t        if(tfidf[i][j] != 0):\n",
    "\t            if words[i] in s.keys():\n",
    "\t                s[words[i]] += tfidf[i][j]\n",
    "\t            else:\n",
    "\t                s[words[i]] = tfidf[i][j]\n",
    "\treturn sorted(s.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
